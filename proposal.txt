Our final project will be to train a multi-limb embodied robot to perform a set of domestic tasks in a simulated house environment. We aim to combine a top-level Vision-Language Model (VLM) agent with a set of discrete, low-level navigational and manipulation tasks to create a Hierarchical Reinforcement Learning (HRL) system. 

To implement this project, we will utilize an existing 3D simulation framework built in Python, called Habitat 2.0 by FacebookResearch, as our environment. We will build specific tasks for our agent using this open-source library, then build the HRL framework. We plan to use PyTorch to code and train  different RL agents for each individual low-level navigation task, evaluating different algorithms for each task, and analyzing ideal hyperparameter settings for each. 

Our project aims to develop a hierarchal control framework to enable a robot to perform goal-oriented tasks within a simulated environment. The primary goal of the robot will be to navigate towards a specified target and position its manipulator towards the object. Ultimately, the model should be able to do more advanced actions, such as picking an object up and moving towards a receptable.  

We expect the robot to consist of at least a unicycle driving base, a front-facing camera, and a multi-joint arm. While the exact implementation  of the robot is subject to how complex we want the integration  with the environment to be, we will start with a robot whose actions consist of moving forward, backward, left and right; moving its arm with 2 degrees of freedom; and moving its claw with 2 degrees of freedom. The input state of the robot will be the view from its front facing camera, an absolute state of each of its joints (1 for each degree of freedom), an absolute state of its position, and specific absolute attributes of its environment (ex: the exact weight of an object in the environment that it might  be learning to pick up). The specific attributes of the environment will come from an existing Habitat 2.0 labeled dataset from ReplicaCAD, and we expect it to help with training. As we being training and testing, the set of allowed actions may change (for example, the 4-cardinal direction driving might become a continuous linear/angular velocity; or we may decrease the degrees of freedom of one part of the arm). Further, the set of input states may also change (for example, we may remove absolute GPS position; or add multiple frames from the FOV camera). 

The project components are divided into three main layers: the low-level skill training layer, the high-level translation layer, and the evaluation layer. We will explore a myriad of RL algorithms to achieve the three main subtasks which make up the skill layer, i.e., the subtasks of navigation, reaching, and, ultimately, picking. For these subtasks, we define them as episodic Markov tasks – such that there is a concrete positive reward for each episode that successfully reached a specific terminal state. They are as follows: For the navigation, the robot’s ending position must be with a certain  arbitrary threshold (determined as a feature of the environment decided after initial experimentation of the codebase) of its target position; For reaching, the robot’s arm-end receptor to be within a certain threshold; And for picking/grabbing, the robot’s claw must have complete grasp of the target object such that it cannot be removed from the claw without an additional claw movement. 

While training these subtasks, we may employ a more granular definition of the reward function, as the existing Habitat 2.0 library defines more complex rewards for multi-embodied cognition (an example of this is an existing reward function for the grabbing task that outputs a reward proportional to the grasp level of the claw taking into account the density and shape of the object). Choosing to use these granular level reward functions  will be another experiment  we conduct during training, while also comparing the different RL algorithms implemented for each subtask. 

The high-level translation layer will employ vision-language models (OWL-ViT or YOLOv8) to detect relevant objects on top of a lightweight LLM (such as GPT-4 or Llama 3) to translate a natural language prompt into a sequence of low-level subgoals. Lastly, the evaluation layer will provide evaluation metrics for end-to-end and episodic success. 

Tools/Environment
The actual project implementation will be a Python project. It will load a simulator using Habitat 2.0 (Habitat-Lab) (https://github.com/facebookresearch/habitat-lab). We will use OpenAI’s Gymnasium library to step into the Habitat environment and read from the environment. We will use Stable-Baseline3 to implement the foundational RL algorithms, including Deep Q Learning (DQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC). We will utilize ReplicaCAD, which has a labeled data set of manipulatable objects in the Habitat 2.0 environment. This labeled dataset will give us specific additional state parameters to pass to the RL agents during training  so that  it has additional  information  to use while  training.  These specific  parameters augment the reward function pre-loaded into the environment of Habitat 2.0. We will run training of the tasks on the WPI Turing Cluster to utilize its GPUs. The top level VLM-to-plan agent will likely be a DSPY agent using an OpenAI LLM model.  

By the project’s conclusion, we expect to: 
- Train at least two working skill policies (navigate, reach). 
    - Have a concrete analysis of which RL algorithms are best suited for each task, show how specific changes in action/state space affect the training pipeline, and present an overview of the optimized policies. 
- Integrate them hierarchically for sequential task completion. 
- Demonstrate successful multi-stage manipulation behavior. 
- Optionally extend to a natural-language command interface. 