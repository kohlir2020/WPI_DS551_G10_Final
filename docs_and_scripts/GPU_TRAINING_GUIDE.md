# GPU Docker Training Guide - ARM REACHING

## ðŸš€ Quick Start (30 seconds)

```bash
# Make script executable
chmod +x start_gpu_training.sh

# Run setup (builds Docker image, starts container, shows training commands)
./start_gpu_training.sh

# Then run training in another terminal
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 100000 \
  --device cuda
```

---

## ðŸ“‹ Full Training Workflow

### Step 1: Setup GPU and Docker

```bash
# Check GPU
nvidia-smi

# Check CUDA version (should be 11.8+)
nvcc --version

# Verify Docker GPU support
docker run --rm --runtime=nvidia nvidia/cuda:11.8.0-base nvidia-smi
```

### Step 2: Start Container

```bash
# Run setup script (recommended)
chmod +x start_gpu_training.sh
./start_gpu_training.sh

# Or manually start
docker-compose up -d hrl-training

# Verify container running
docker ps -f name=hrl-training

# Check GPU inside container
docker exec hrl-training nvidia-smi
```

### Step 3: Train Models

#### A. Train PPO (Recommended First)
```bash
# Standard training (100k steps)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 100000 \
  --device cuda

# Extended training (200k steps for better convergence)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 200000 \
  --device cuda
```

#### B. Train A2C (Actor-Critic)
```bash
# Standard training
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm A2C \
  --steps 100000 \
  --device cuda

# Note: A2C typically converges slower than PPO
```

#### C. Train SAC (Soft Actor-Critic)
```bash
# Standard training (SAC is sample-efficient, can use fewer steps)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm SAC \
  --steps 100000 \
  --device cuda

# SAC can also be trained longer
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm SAC \
  --steps 200000 \
  --device cuda
```

#### D. Compare All Algorithms (Sequential)
```bash
# Train PPO, A2C, SAC sequentially (50k steps each)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --compare \
  --compare-steps 50000 \
  --device cuda

# This will take ~45-60 minutes total
# Logs will be in logs/fetch_arm/{algorithm}_{timestamp}/
```

---

## ðŸ“Š Monitoring Training

### Real-time TensorBoard (Recommended)

```bash
# In a separate terminal
tensorboard --logdir logs/fetch_arm

# Then open browser: http://localhost:6006
```

### Watch GPU Utilization

```bash
# Update every 1 second during training
watch -n 1 'docker exec hrl-training nvidia-smi'

# Or just once
docker exec hrl-training nvidia-smi
```

### View Training Logs

```bash
# Tail latest TensorBoard events
docker exec hrl-training tail -f logs/fetch_arm/*/events.out.tfevents*

# View container logs
docker logs -f hrl-training

# Check checkpoint files
docker exec hrl-training ls -lah logs/fetch_arm/*/checkpoints/
```

---

## ðŸ§ª Experiments & Workflow

### Experiment 1: PPO Baseline (Start Here)

```bash
# Train PPO for baseline
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 100000 \
  --device cuda

# Monitor with TensorBoard
tensorboard --logdir logs/fetch_arm
```

**Expected Results:**
- Training time: ~10 minutes (GPU), ~40 minutes (CPU)
- Success rate: 0% â†’ 30% â†’ 60% â†’ 80%+
- Final model: `logs/fetch_arm/ppo_*/arm_ppo_final.zip`

### Experiment 2: Compare RL Algorithms

```bash
# Train all three algorithms (50k steps each)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --compare \
  --compare-steps 50000 \
  --device cuda

# Or train individually and compare results later
docker exec hrl-training python src/arm/train_arm_multiagent.py --algorithm PPO --steps 50000
docker exec hrl-training python src/arm/train_arm_multiagent.py --algorithm A2C --steps 50000
docker exec hrl-training python src/arm/train_arm_multiagent.py --algorithm SAC --steps 50000
```

**Comparison Points:**
- Which converges fastest?
- Which achieves highest success rate?
- GPU memory usage per algorithm?
- Training stability (reward smoothness)?

**Results Location:**
```
logs/fetch_arm/
â”œâ”€â”€ ppo_YYYYMMDD_HHMMSS/
â”‚   â”œâ”€â”€ arm_ppo_final.zip
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ events.out.tfevents...
â”œâ”€â”€ a2c_YYYYMMDD_HHMMSS/
â”‚   â””â”€â”€ ...
â””â”€â”€ sac_YYYYMMDD_HHMMSS/
    â””â”€â”€ ...
```

### Experiment 3: Vision-Based Observations

(For next phase after PPO baseline works)

```bash
# Create vision-based training script (similar to train_arm_multiagent.py)
# Instead of: observation = [distance, arm_angles(7)]
# Use: observation = stacked_RGB_images

# Training command (when vision trainer is ready)
docker exec hrl-training python src/arm/train_vision_arm.py \
  --algorithm PPO \
  --image-size 64 \
  --frame-stack 4 \
  --steps 100000 \
  --device cuda
```

---

## ðŸ’¾ Model Management

### Save and Load Models

```bash
# Models are automatically saved to:
# logs/fetch_arm/{algorithm}_{timestamp}/arm_{algorithm}_final.zip

# List all trained models
docker exec hrl-training find logs/fetch_arm -name "*.zip" -type f

# Create models directory symlink for easy access
docker exec hrl-training ln -sf ../logs/fetch_arm models/arm_models
```

### Evaluate Trained Model

```bash
# Create evaluation script to test saved model
docker exec hrl-training python -c "
from stable_baselines3 import PPO
from fetch_arm_reaching_env import FetchArmReachingEnv

# Load model
model = PPO.load('logs/fetch_arm/ppo_YYYYMMDD_HHMMSS/arm_ppo_final')

# Test
env = FetchArmReachingEnv()
obs, _ = env.reset()

for _ in range(200):
    action, _ = model.predict(obs)
    obs, reward, done, _, info = env.step(action)
    if done:
        print(f'Success! Distance: {info[\"distance\"]:.3f}m')
        break

env.close()
"
```

---

## ðŸ”§ Advanced Options

### Custom Training Parameters

```bash
# PPO with custom learning rate
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 200000 \
  --device cuda
  # Edit train_arm_multiagent.py to change lr, batch_size, etc.

# SAC with different buffer size
# Edit config in train_arm_multiagent.py:
# "buffer_size": 200_000  # Increase for longer training
```

### CPU-only Training

```bash
# Useful for debugging or if GPU not available
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO \
  --steps 10000 \
  --device cpu

# Much slower but useful for testing
```

---

## ðŸ“ˆ Expected Training Times

| Algorithm | Steps | GPU (NVIDIA 4090) | GPU (NVIDIA 3080) | CPU (Xeon) |
|-----------|-------|------------------|------------------|-----------|
| PPO | 50k | 5 min | 8 min | 20 min |
| PPO | 100k | 10 min | 16 min | 40 min |
| PPO | 200k | 20 min | 32 min | 80 min |
| A2C | 50k | 6 min | 10 min | 22 min |
| SAC | 50k | 7 min | 12 min | 28 min |

---

## ðŸŽ¯ Performance Expectations

### PPO Results (after training)
- Success rate: 70-85%
- Average reward: 0.65-0.75
- Training stability: High
- Inference speed: <5ms per step

### A2C Results
- Success rate: 60-75%
- Average reward: 0.55-0.65
- Training stability: Medium
- Inference speed: <5ms per step

### SAC Results
- Success rate: 65-80%
- Average reward: 0.60-0.70
- Training stability: Medium-High
- Inference speed: <5ms per step

---

## ðŸ›‘ Stopping & Cleanup

### Stop Training

```bash
# Graceful stop (Ctrl+C in training terminal)
# Model will be saved before exit

# Force stop
docker kill hrl-training

# Stop container
docker-compose down
```

### Clean Up

```bash
# Remove container
docker-compose down

# Remove image
docker rmi hrl-training:latest

# Remove logs
docker exec hrl-training rm -rf logs/fetch_arm/*

# Remove all Docker artifacts
docker system prune -a
```

---

## ðŸ› Troubleshooting

### GPU Not Available

```bash
# Check if GPU is visible in Docker
docker exec hrl-training nvidia-smi

# If not visible, check:
docker run --rm --runtime=nvidia nvidia/cuda:11.8.0-base nvidia-smi

# Install nvidia-docker if needed:
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
```

### Out of Memory

```bash
# Reduce batch size in train_arm_multiagent.py
# Change: "batch_size": 32  # Instead of 64

# Or use CPU mode
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO --steps 10000 --device cpu
```

### Docker Container Won't Start

```bash
# Check logs
docker logs hrl-training

# Try rebuild
docker-compose build --no-cache hrl-training

# Or use setup script
./start_gpu_training.sh --rebuild
```

### TensorBoard Not Working

```bash
# Make sure port is forwarded
docker ps -f name=hrl-training --format "table {{.Ports}}"

# Should show: 0.0.0.0:6006->6006/tcp

# Restart if needed
docker-compose restart hrl-training
tensorboard --logdir logs/fetch_arm --port 6006
```

---

## ðŸ“š Next Steps After PPO Training

### 1. Evaluate Models
- Compare success rates across algorithms
- Visualize learned policies
- Analyze failure modes

### 2. Implement Vision-Based Training
- Use RGB camera observations
- Train with CNN policy instead of MLP
- Compare performance vs. low-level state

### 3. Hyperparameter Tuning
- Fine-tune learning rates
- Adjust network sizes
- Optimize batch sizes

### 4. Integration with Navigation
- Combine navigation + arm reaching
- Sequential execution
- End-to-end RL system

---

## ðŸ“ž Quick Reference

```bash
# Start everything
./start_gpu_training.sh

# Train PPO (standard)
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --algorithm PPO --steps 100000 --device cuda

# Monitor
tensorboard --logdir logs/fetch_arm

# Compare all algorithms
docker exec hrl-training python src/arm/train_arm_multiagent.py \
  --compare --compare-steps 50000 --device cuda

# Stop
docker-compose down
```

---

**Status: âœ… READY FOR GPU TRAINING**

Run `./start_gpu_training.sh` to begin!
